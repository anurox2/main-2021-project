# -*- coding: utf-8 -*-
"""Autoencoder_BCROSS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O3RqiUq_Wi5BGrnxYlLS5UZmgK6OBbyq
"""

# Data manipulation libraries
import pandas as pd
from numpy import array
import numpy as np
import os
from datetime import datetime

# train autoencoder for classification with no compression in the bottleneck layer
from sklearn.datasets import make_classification
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.utils import plot_model
from tensorflow.keras import losses
from keras.optimizers import Adam

# Plotting
from matplotlib import pyplot
import matplotlib.pyplot as plt

# Loading saved model
from keras.models import load_model

"""---
## Loading data ---------------------------------------------------------------------------------------------------------------------------
"""

## Sequenced data (chrome, firefox and then cloudflared)
# main_data = pd.read_csv('doh-main-numbers.csv')

## Randomized data
main_data = pd.read_csv('randomized_doh_data.csv')

print("Number of records -", main_data.shape)

"""---
## Select options -------------------------------------------------------------------------------------------------------------------------
"""
parent_dir = os.getcwd()
current_datetime = str(datetime.now().strftime("%d%m%Y-%H%M%S"))
dir_name = parent_dir+"/"+current_datetime+"_RANDOM"

print("Creating directory", dir_name)
os.mkdir(dir_name)
os.chdir(dir_name)

# Data scaling
scale = True
scaling_range = (0,100)
epochs = 50
neurons = 3

loss_function = 'mse'
# loss_function = losses.mean_squared_logarithmic_error
# loss_function = 'binary_crossentropy'

batch_size = 16

if (scale):
  if(loss_function == losses.mean_squared_logarithmic_error):
    filename = str(loss_function)[10:40]+"_N_"+str(neurons)+"_scaled_"+str(scaling_range[0])+"-"+str(scaling_range[1])
  else:
    filename = loss_function+"_N_"+str(neurons)+"_scaled_"+str(scaling_range[0])+"-"+str(scaling_range[1])
else:
  if(loss_function == losses.mean_squared_logarithmic_error):
    filename = str(loss_function)[10:40]+"_N_"+str(neurons)
  else:
    filename = loss_function+"_N_"+str(neurons)

"""---
## Reshape input into [samples, timesteps, features] --------------------------------------------------------------------------------------
"""

X = main_data.drop(columns=['is_doh']).to_numpy()
y = main_data['is_doh'].to_numpy()

# Split the data into test and train
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)

"""---
# Scale the data --------------------------------------------------------------------------------------------------------------------------
"""
if (scale):
  scaler = MinMaxScaler(feature_range=scaling_range)
  scaler.fit(X_train)
  X_train = scaler.transform(X_train)
  X_test = scaler.transform(X_test)
else:
  pass

# Encoder design
number_of_inputs = X.shape[1]
input_encoder = Input(shape=(number_of_inputs,))

## Encoder level 1
encoder_1 = Dense(number_of_inputs*2)(input_encoder)
encoder_1 = BatchNormalization()(encoder_1)
encoder_1 = LeakyReLU()(encoder_1)

## Encoder level 2
encoder_2 = Dense(number_of_inputs)(encoder_1)
encoder_2 = BatchNormalization()(encoder_2)
encoder_2 = LeakyReLU()(encoder_2)

divisor = number_of_inputs/neurons

# Bottleneck design
bottleneck_inputs = number_of_inputs/divisor
bottleneck = Dense(bottleneck_inputs)(encoder_2)

# Decoder design
## Decoder Level 1
decoder_1 = Dense(number_of_inputs)(bottleneck)
decoder_1 = BatchNormalization()(decoder_1)
decoder_1 = LeakyReLU()(decoder_1)

## Decoder Level 2
decoder_2 = Dense(number_of_inputs*2)(decoder_1)
decoder_2 = BatchNormalization()(decoder_2)
decoder_2 = LeakyReLU()(decoder_2)


output = Dense(number_of_inputs, activation='linear')(decoder_2)

# Defining the autoencoder model
model = Model(inputs=input_encoder, outputs=output)

"""---
## Compiling the model -------------------------------------------------------------------------------------------------------------------
"""

# model.compile(optimizer='adam', loss=loss_function)
model.compile(Adam(0.001), loss=loss_function)

# Plotting the autoencoder
plot_model(model, "autoencoder_"+filename+".png", show_shapes=True)

history = model.fit(X_train, 
                    X_train,
                    epochs=epochs,
                    batch_size=batch_size,
                    verbose=1,
                    validation_data=(X_test, X_test))

"""---
## Plotting the loss and val_loss graph VS Epochs ----------------------------------------------------------------------------------------
"""

pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
# pyplot.show()
pyplot.savefig("training_"+filename+".png")

# define an encoder model (without the decoder)
encoder_no_decoder = Model(inputs=input_encoder, outputs=bottleneck)
plot_model(encoder_no_decoder, 'encoder_'+filename+'.png', show_shapes=True)

# save the encoder to file
encoder_no_decoder.save('encoder_no_decoder_'+filename+'.h5')
model.save('full_autoencoder_'+filename+'.h5')

"""---
## Loading model back in ------------------------------------------------------------------------------------------------------------------
"""

model_encoder_only = load_model('encoder_no_decoder_'+filename+'.h5')
model_encoder_only.compile(optimizer='adam', loss=loss_function)

# Copying data
X_pred = X_test
y_pred = y_test

"""---
## Running prediction ---------------------------------------------------------------------------------------------------------------------
"""

prediction_output = model_encoder_only.predict(X_pred)
print("######## PREDICTION values ########")
print(prediction_output)

print("######## ACTUAL values ########")
print(y_pred)

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

x_coord = []
y_coord = []
z_coord = []
for i in range(0, len(prediction_output)):
  x_coord.append(prediction_output[i][0])
  y_coord.append(prediction_output[i][1])
  z_coord.append(prediction_output[i][2])

def plotcolor(coord_list):
  colors = []
  for item in coord_list:
    if item == 1:
      colors.append('red')
    else:
      colors.append('green')
  return colors

plt_colors = plotcolor(y_pred)

# ax.scatter(x_coord, y_coord, c=plt_colors)

# ax.set_xlabel('X Label - Pred')
# ax.set_ylabel('Y Label - Pred')

# plt.savefig('prediction_'+filename+'.png')

ax.scatter(x_coord, y_coord, z_coord, c=plt_colors)

ax.set_xlabel('X Label - Pred')
ax.set_ylabel('Y Label - Pred')
ax.set_zlabel('Z Label - Actual')

plt.savefig('prediction_Z_'+filename+'.png')